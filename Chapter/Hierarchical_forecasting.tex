%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

% choose options for [] as required from the list
% in the Reference Guide

\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index

\usepackage[bottom]{footmisc}% places footnotes at page bottom

\usepackage{qtree, bm, amsmath, amssymb, qtree, bm, multirow, textcmds, siunitx, mathrsfs, float, booktabs, color, soul}
\usepackage{natbib, setspace}
\usepackage{pdfpages} %To insert pdf pages
\usepackage[bb=boondox]{mathalfa}
\usepackage{tikz}
\definecolor{hfill_blue}{RGB}{208,229,249}
\definecolor{hfill_yellow}{RGB}{255,255,208}
\usetikzlibrary{trees,shapes}
\usetikzlibrary{matrix}
\tikzstyle{line} = [draw, thick]

% see the list of further useful packages
% in the Reference Guide

\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\textsc{\textsc{\bibliographystyle{natbb}
%
%\bibliography{References_HTSF}

\def\ba{\begin{pmatrix}\tilde{\bm{b}}\\ \tilde{\bm{a}}\end{pmatrix}}
\def\GH{\begin{pmatrix}\bm{G}\\ \bm{F}\end{pmatrix}}
\def\Naive{Na\"{i}ve\ }
\def\naive{na\"{i}ve\ }



\begin{document}

\title*{Hierarchical Forecasting}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\author{Name of First Author and Name of Second Author}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\institute{Name of First Author \at Name, Address of Institute, \email{name@email.address}
\and Name of Second Author \at Name, Address of Institute \email{name@email.address}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle

\section{Introduction}\label{sec:intro}
			
Accurate forecasting of key macroeconomic variables such as Gross Domestic Product (GDP), inflation, industrial production, has been at the forefront of economic research over many decades. Early approaches involved univariate models or at best low dimensional multivariate systems. The era of big data has now led to the use of regularization and shrinkage methods such as dynamic factor models, Lasso, LARS, Bayesian VARs, in an effort to exploit the plethora of potentially useful predictors now available. These predictors commonly also include the components of the variables of interest.  For instance, GDP is formed as an aggregate of consumption, government expenditure, investment and net exports with each of these components also formed as aggregates of other economic variables.  While the macroeconomic forecasting literature regularly uses such sub-indices as predictors, it does so in ways that fail to exploit accounting identities that describe known deterministic relationships between macroeconomic variables. 

In this chapter we take a different approach.  Over the past decade there has been a growing literature on forecasting collections of time series that follow aggregation constraints, known as hierarchical times series.  Initially the aim of this literature was to ensure that forecasts adhered to aggregation constraints thus ensuring aligned decision making.  However in applications to tourism data, retail data, {\color{red} more  examples and references} the forecast reconciliation methods designed to deal with this problem have also been shown to improve forecast accuracy.  Since both aligned decision making and forecast accuracy are key concerns for economic agents and policy makers we propose the application of state of the art forecast reconciliation methods to macroeconomic forecasting. To the best of our knowledge the only application of forecast reconciliation methods to macroeconomics is the PhD thesis of XXX which focuses on point forecasting for inflation. {\color{red} Are we aware of any other papers that apply forecast reconciliation methods to macroeconomic data?}

The remainder of the paper is set out as follows.  Section 2 introduces the concept of hierarchical time series, i.e. collections of time series with known linear constraints, with a particular emphasis on macroeconomic examples. Section 3 describes state-of-the-art forecast reconciliation techniques for point forecasts, while section 4 describes the more recent extension of these techniques to probabilistic forecasting.  Section 5 describes the data used in our empirical case study, namely Australian GDP data that can be represented using two different hierarchies.  Section 6 provides details on the setup of our empirical study including metrics used for the evaluation of both point and probabilistic forecasts.  Section 7 presents results and Section 8 concludes providing future avenues for research that are of particular relevance to macroeconometrians.

\section{Hierarchical time series}\label{sec:Hier ts}

To simplify the introduction of some notation we use the simple two-level hierarchical structure shown in Figure \ref{fig:simple tree}. Denote as $y_{Tot,t}$ the value observed at time $t$ for the most aggregate (Total) series  corresponding to level 0 of the hierarchy. Below level 0, denote as $y_{i,t}$ the value of the series corresponding to node $i$, observed at time $t$. For example, $y_{A,t}$ denotes the $t$th observation of the series corresponding to node A at level 1, $y_{AB,t}$ denotes the $t$th observation of the series corresponding to node AB at level 2, and so on.

\begin{figure}[!hbt]  \center
  \begin{tikzpicture}
    \tikzstyle{every node}=[ellipse,draw,inner sep=2pt,minimum size=7mm,fill=red!15] %158,202,225
    \tikzstyle[level distance=.1cm]
    \tikzstyle[sibling distance=.1cm]
    %\tikzstyle{level 3}=[sibling distance=6.2mm,font=\tiny]
    \tikzstyle{level 1}=[sibling distance=40mm, font=\small, set style={{every node}+=[fill=hfill_blue]}]
    \tikzstyle{level 2}=[sibling distance=15mm, font=\footnotesize, set style={{every node}+=[fill=hfill_yellow]}]
    \node{Total}%[edge from parent fork down]
    child {node {A}
      child {node {AA}}
      child {node {AB}}
    }
    child {node {B}
      child {node {BA}}
      child {node {BB}}
      child {node {BC}}
    };
  \end{tikzpicture}
  \caption{A simple two-level hierarchical structure.}
  \label{fig:simple tree}
\end{figure}

Let $\bm{y}_t = (y_{Tot,t},y_{A,t}, y_{B,t},y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t},y_{BC,t})'$, a vector containing observations across all series of the hierarchy at $t$. Similarly denote as \linebreak $\bm{b}_t = (y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}, y_{BC,t})'$ a vector containing observations only for the bottom-level series. In general, $\bm{y}_t\in \mathbb{R}^n$ and $\bm{b}_t \in \mathbb{R}^m$ where $n$ denotes the number of total series in the structure, $m$ the number of series at the bottom level, and $n>m$ always. In the simple example of Figure \ref{fig:simple tree}, $n=8$ and $m=5$.

Aggregation constraints dictate that $y_{Tot}=y_{A,t}+y_{B,t}=y_{AA,t}+y_{AB,t}+y_{BA,t}+y_{BB,t}+y_{BC,t}$,~ $y_{A,t}=y_{AA,t}+y_{AB,t}$ and $y_{B}=y_{BA,t}+y_{BB,t}+y_{BC,t}$. Hence we can write
\begin{equation}\label{eq:summing matrix}
\bm{y}_t = \bm{Sb}_t,
\end{equation}
where \begin{equation*}
\bm{S} = \begin{pmatrix}
1& 1& 1& 1 & 1 \\
1& 1& 0& 0 & 0\\
0& 0& 1& 1 & 1\\
& \multicolumn{3}{c}{\bm{I}_5} &
\end{pmatrix}
\end{equation*}
an $n\times m$ matrix referred to as the \textit{summing matrix} and $\bm{I}_m$ is an $m$-dimensional identity matrix. $\bm{S}$ reflects the linear aggregation constraints and in particular how the bottom-level series aggregate to levels above. Thus, columns of $\bm{S}$ span the linear subspace of $\mathbb{R}^n$ for which the aggregation constraints hold. We refer to this as the \textit{coherent subspace} and denote it by $\mathfrak{s}$. Notice that pre-multiplying a vector in $\mathbb{R}^m$ by $\bm{S}$ will result in an $n$-dimensional vector that lies in $\mathfrak{s}$.

\begin{property}
A hierarchical time series has observations that are \textit{coherent}, i.e., $\bm{y}_{t} \in \mathfrak{s}$ for all $t$. We use the term coherent to describe not just $\bm{y}_t$ but any vector in $\mathfrak{s}$.
  \label{prop:coherence}
\end{property}


Structures similar to the one portrayed in Figure \ref{fig:simple tree} can be found in macroeconomics. For instance in Section~\ref{sec:data} we consider two alternative hierarchical structures for the case of GDP and its components.  However, while this motivating example involves aggregation constraints, the mathematical framework we use can be applied for any general linear constraints, examples of which are ubiquitous in macroeconomics. For instance, the trade balance is computed as exports minus imports, while the consumer price index is computed as a weighted average of sub-indices, which are in turn weighted averages of sub-sub-indices and so on.  These structures can also be captured by an appropriately designed $\bm{S}$ matrix.

An important alternative aggregation structure also commonly found in macroeconomics, is one for which the most aggregate series is disaggregated by attributes of interest that are crossed, as distinct to nested which is the case for hierarchical time series. For example, industrial production may be disaggregated along the lines of geography or sector or both. We refer to this as a \textit{grouped} structure. Figure \ref{fig:simple grouped tree} shows a simple example of such a structure. The Total series disaggregates into $y_{A,t}$ and $y_{B,t}$, but also into $y_{X,t}$ and $y_{Y,t}$, at level 1, and then into the bottom-level series, $\bm{b}_t=(y_{AX}, y_{AY}, y_{BX}, y_{BY})'$. Hence, in contrast to hierarchical, grouped time series do not naturally disaggregate in a unique manner.
\begin{figure}[!hbt]
\center
\tikzstyle{every node}=[inner sep=2pt,minimum size=7mm]
\begin{tikzpicture}
    \matrix[ampersand replacement=\&,column sep=0.3cm] {
        \node[circle,draw,fill=hfill_yellow, font=\small,distance=1cm] {AX};~ \&
        \node[circle,draw,fill=hfill_yellow,font=\small] {AY};~ \&
        \node[circle,draw,fill=hfill_blue, font=\small] {A}; \\[0.3cm]
        \node[circle,draw,fill=hfill_yellow, font=\small] {BX};~ \&
        \node[circle,draw,fill=hfill_yellow, font=\small] {BY};~ \&
        \node[circle,draw,fill=hfill_blue, font=\small] {B}; \\[0.3cm]
        \node[circle,draw,fill=hfill_blue, font=\small] {X};~ \&
        \node[circle,draw,fill=hfill_blue, font=\small] {Y};~ \&
        \node[ellipse,draw,fill=red!15] {Total}; \\
};
\end{tikzpicture}
  \caption{A simple two-level grouped structure.}
  \label{fig:simple grouped tree}
\end{figure}

An important implementation of aggregation structures are \textit{temporal hierarchies} introduced by \cite{AthEtAl2017}. In this case the aggregation structure spans the time dimension and dictates how higher frequency data (e.g., monthly) are aggregated to lower frequencies. There is a vast literature that studies the effects of temporal aggregation, going back to the seminal work of \cite{ZelMon1971, AmeWu1972, Tia1972, Bre1973} and others such as, \cite{Hot1993, HotCar1993, Mar1999, SilEtAl2008}. The main aim of this work is to find the single most optimum level of aggregation for modelling and forecasting time series. In this literature, the analyses, results (whether theoretical or empirical) and inferences, are extremely heterogeneous, making it very challenging to reach a consensus or some concrete conclusions. For example, \cite{RosSea1995} who study the effect of aggregation on several key macroeconomic variables state, ``Quarterly data do not seem to suffer badly from temporal aggregation distortion, nor are they subject to the construction problems affecting monthly data. They therefore may be the optimal data for econometric analysis.'' A similar conclusion is reached by \cite{NijPal1990}. \cite{SilEtAl2008} consider forecasting French cash state deficit and provide empirical evidence of forecast accuracy gains from forecasting with the aggregate model rather than aggregating forecasts from the disaggregate model.

The overwhelming majority of the literature concentrates on a single level of temporal aggregation \citep[there are some notable exceptions such as,][]{AndEtAl2011,KouEtAl2014}. \cite{AthEtAl2017} show that considering multiple levels of aggregation via temporal hierarchies and implementing forecast reconciliation approaches rather than single level approaches results in substantial gains in forecast accuracy across all levels of temporal aggregation. This is an example of the benefits of forecast reconciliation to which we now turn out attention to.

\section{Point forecasting}\label{sec:point forecasting}

A requirement when forecasting hierarchial time series is that the forecasts adhere to the same aggregation constraints as the observed data, i.e., they are coherent.

\begin{definition}
A set of $h$-step ahead forecasts $\tilde{\bm{y}}_{T+h|T}$, stacked in the same order as $\bm{y}_{t}$ and generated using information up to and including time $T$,
are said to be \textit{coherent} if $\tilde{\bm{y}}_{T+h|T} \in \mathfrak{s}$.
  \label{def:coherence}
\end{definition}

Hence, coherent forecasts of lower level series aggregate up to their corresponding upper level series and vice versa.

\begin{figure}[t]
	\centering
	\vspace{-0.9cm}
	\small
	\resizebox{\linewidth}{!}{
	\input{Figs/Diagram_3D/Schematic_3D}
}
	\caption{Representation of coherent subspace in a three dimensional hierarchy where $y_{Tot} = y_A + y_B$. The coherent subspace is depicted as a gray two dimensional plane labeled $\mathfrak{s}$. Note the columns of $\vec{s}_1 = (1,1,0)'$ and $\vec{s}_2 = (1, 0, 1)'$ form a basis for $\mathfrak{s}$. The red points lying on $\mathfrak{s}$ can be either realisations or coherent forecasts. }\label{fig:Schematic-3D}

\end{figure}

Let us consider the smallest possible hierarchy with two bottom level series, depicted in Figure\ref{fig:Schematic-3D}, where $y_{Tot} = y_A + y_B$. While base forecasts could lie anywhere in $\mathbb{R}^3$, the realisation and coherent forecasts lie in a two dimensional subspace $\mathfrak{s}\subset \mathbb{R}^3$.

\subsection{Single-level approaches}\label{sec:single level approaches}
A common theme across all traditional approaches for forecasting hierarchical time series is that a single-level of aggregation is first selected and forecasts for that level are generated. These are then linearly combined to generate a set of coherent forecasts the rest of the structure.

\subsubsection{Bottom-up}

In the \textit{bottom-up} approach, forecasts for the most disaggregate level are first generated. These are then aggregated to obtain forecasts for all other series of the hierarchy \citep{dunn1976}. In general, this consists of first generating $\hat{\bm{b}}_{T+h|T} \in \mathbb{R}^m$, a set of $h$-step ahead forecasts for the bottom-level series. For the simple hierarchical structure of Figure \ref{fig:simple tree}, $\hat{\bm{b}}_{T+h|T} = (\hat{{y}}_{AA,T+h|T}, \hat{{y}}_{AB,T+h|T}, \hat{{y}}_{BA,T+h|T}, \hat{{y}}_{BB,T+h|T},\hat{{y}}_{BC,T+h|T}),$ where, $\hat{{y}}_{i,T+h|T}$ is the $h$-step ahead forecast of the series corresponding to node $i$. A set of coherent forecasts for the whole hierarchy is then given by,
\begin{equation*}\label{eq:BU}
\tilde{\bm{y}}^{BU}_{T+h|T}=\bm{S\hat{\bm{b}}}_{T+h|T}.
\end{equation*}
Generating bottom-up forecasts has the advantage of no information being lost due to aggregation. However, bottom-level data can potentially be highly volatile or very noisy and therefore challenging to forecast.

\subsubsection{Top-down}

In contrast \textit{top-down} approaches involve first generating forecasts for the most aggregate level and then disaggregating these down the hierarchy. In general, coherent forecasts generated from top-down approaches are given by,
\begin{equation*}
\tilde{\bm{y}}^{TD}_{T+h|T}=\bm{S}\bm{p}\hat{y}_{Tot, T+h|T},
\end{equation*}
where $\bm{p} = (p_1,...,p_m)'$ is an $m$-dimensional vector consisting of a set of proportions which disaggregate the top-level forecast $\hat{y}_{Tot, T+h|T}$ to forecasts for the bottom-level series, hence $\bm{p}\hat{y}_{Tot, T+h|T}=\bm{\hat{\bm{b}}}_{T+h|T}$. These are then aggregated up by the summing matrix $\bm{S}$.

Traditionally proportions have been calculated based on the observed historical data. \cite{gross1990} present and evaluate twenty-one alternative approaches. The most convenient attribute of these approaches is their simplicity. Generating a set of coherent forecasts involves only modelling and generating forecasts for the most aggregate top-level series. In general, such top-down approaches seem to produce quite reliable forecasts for the aggregate levels and they are useful with low count data. However, a significant disadvantage is the loss of information due to aggregation. A limitation of such top-down approaches, is that characteristics of lower level series cannot be captured. To overcome this, \cite{AthEtAl2009} introduced a new top-down approach which disaggregates the top-level based on proportions of forecasts rather than the historical data and show evidence that this method outperforms the conventional top-down approaches. However, a limitation of all top-down is that they introduce bias to the forecasts even when the top-level forecast itself is unbiased. We discuss this in detail in Section \ref{sec:reconciliation approaches} that follows.

\subsubsection{Middle-out}

A compromise between bottom-up and top-down approaches is the middle-out approach. It entails first forecasting the series of a selected middle-level. For series above the middle-level, coherent forecasts are generated using the bottom-up approach by aggregating the middle-level forecasts upwards. For series below the middle level, coherent forecasts are generated using a top-down approach by disaggregating the middle-level forecasts downwards. Since the middle-out approach involves generating top-down forecasts, it also introduces bias to the forecasts.



\subsection{Point forecast reconciliation}\label{sec:reconciliation approaches}

All approaches discussed so far are limited to only using information from a single-level of aggregation. Furthermore, these ignore any correlations across levels of a hierarchy. An alternative framework that overcomes these limitations is one that involves forecast \textit{reconciliation}. In a first step forecasts for all the series across all levels of the hierarchy are generated, ignoring any aggregation constraints. We refer to these as \textit{base} forecasts and denote them by $\hat{\bm{y}}_{T+h|T}$. In general, base forecasts will not be coherent, although a notable exception is when a random walk is used to generate base forecasts. In this case, forecasts are simply equal to a previous realisation of the data and they inherit the property of coherence.

The second step is an ex-post adjustment that reconciles base forecasts so that they become coherent. In general, this is achieved by mapping the base forecasts $\hat{\bm{y}}_{T+h|T}$ onto the coherent subspace $\mathfrak{s}$, via a matrix $\bm{SG}$, resulting in a set of coherent forecasts $\tilde{\bm{y}}_{T+h|T}$. More specifically,
\begin{equation}\label{eq:recon}
\tilde{\bm{y}}_{T+h|T}=\bm{S}\bm{G}\hat{\bm{y}}_{T+h|T},
\end{equation}
where $\bm{G}$ is an $m\times n$ matrix that maps $\hat{\bm{y}}_{T+h|T}$ to $\mathbb{R}^m$, producing new forecasts for the bottom-level, which are in turn mapped to the coherent subspace by the summing matrix $\bm{S}$. We restrict our attention to projections on $\mathfrak{s}$ in which case $\bm{SGS}=\bm{S}$. This ensures that unbiasedness is preserved, i.e., for a set of unbiased base forecasts reconciled forecasts will also be unbiased.

Note that all single-level approaches discussed so far can also be represented by \eqref{eq:recon} using appropriately designed $\bm{G}$ matrices, however not all of these will be projections. For example for the bottom-up approach, $\bm{G}=\begin{pmatrix}
\bm{0}_{(m \times n-m)} & \bm{I}_m
\end{pmatrix}$ in which case $\bm{SGS}=\bm{S}$. For any top-down approach
$\bm{G}=\begin{pmatrix}
\bm{p} & \bm{0}_{(m \times n-1)}
\end{pmatrix}$, for which case $\bm{SGS}\ne\bm{S}$.


\subsubsection{Optimal MinT reconciliation}

\cite{WicEtAl2019} build a unifying framework for much of the previous literature on forecast reconciliation. We present here a detailed outline of this approach and in turn relate it to previous significant contributions in forecast reconciliation.

Assume that $\hat{\bm{y}}_{T+h|T}$ is a set of unbiased base forecasts, i.e., $E_{1:T}(\hat{\bm{y}}_{T+h|T})= E_{1:T}[\bm{y}_{T+h}|\bm{y}_1,...,\bm{y}_T]$, the true mean with the expectation taken over the observed sample up to time $T$.
Let
\begin{equation}\label{eq:base errors}
\hat{\bm{e}}_{T+h|T} = \bm{y}_{T+h|T}-\hat{\bm{y}}_{T+h|T}
\end{equation}
denote a set of base forecast errors with Var$(\hat{\bm{e}}_{T+h|T})=\bm{W}_h$, and
\begin{equation*}
\tilde{\bm{e}}_{T+h|T} = \bm{y}_{T+h|T}-\tilde{\bm{y}}_{T+h|T}
\end{equation*} denote a set of coherent forecast errors. Lemma 1 in \cite{WicEtAl2019} shows that for any matrix $\bm{G}$ such that $\bm{S}\bm{G}\bm{S}=\bm{S}$, $\text{Var}(\tilde{\bm{e}}_{T+h|T})=\bm{S}\bm{G}\bm{W}_h\bm{S}'\bm{G}'
$. Furthermore Theorem 1 shows that
\begin{equation} \label{eq:MinT}
\bm{G} = (\bm{S}'{\bm{W}}^{-1}_h\bm{S})^{-1}\bm{S}'{\bm{W}}^{-1}_h
\end{equation}
is the unique solution that minimises the tr$[\bm{S}\bm{G}\bm{W}_h\bm{S}'\bm{G}']$ subject to $\bm{S}\bm{G}\bm{S}=\bm{S}$. {\color{red} Is there anything in the following sentence that has not already been said?} MinT is optimal in the sense that given a set of unbiased base forecasts, it returns a set of best linear unbiased reconciled forecasts, using as $\bm{G}$ the unique solution that minimises the trace (hence MinT) of the variance of the forecast error of the reconciled forecasts. 
A significant advantage of the MinT reconciliation solution is that it is the first to incorporate the full correlation structure of the hierarchy via ${\bm{W}}_{h}$. However, estimating ${\bm{W}}_{h}$ is challenging, especially for $h>1$. \citet{WicEtAl2019} present possible alternative estimators for ${\bm{W}}_{h}$ and show that these lead to different $\bm{G}$ matrices. We summarise these below.

\begin{itemize}
    \item Set ${\bm{W}}_{h}=k_h\bm{I}_n$, for all $h$, where $k_{h} > 0$ is a proportionality constant. This simple assumption returns $\bm{G}=(\bm{S}'\bm{S})^{-1}\bm{S}'$ so that the base forecasts are orthogonally projected onto the coherent subspace $\mathfrak{s}$ minimising the Euclidean distance between $\hat{\bm{y}}_{T+h|T}$ and $\tilde{\bm{y}}_{T+h|T}$.  \cite{HynEtAl2011} come to same solution, however from the perspective of the following regression model \begin{equation*}\hat{\bm{y}}_{T+h|T} = \bm{S\ubeta}_{T+h|T} + \bm{\varepsilon}_{T+h|T} \end{equation*} where $\bm{\ubeta}_{T+h|T}=E[\bm{b}_{T+h}|\bm{b}_1,.....,\bm{b}_T]$ is the unknown conditional mean of the bottom-level series and $\bm{\varepsilon}_{T+h|T}$ is the coherence or reconciliation error with mean zero and variance $\bm{V}$. The OLS solution leads to the same projection matrix $\bm{S}(\bm{S}'\bm{S})^{-1}\bm{S}'$, and due to this interpretation we continue to refer to this reconciliation method as OLS. A disadvantage of the OLS solution is that the homoscedastic diagonal entries do not account for the scale differences between the levels of the hierarchy due to aggregation.  Furthermore, OLS does not account for the correlations across series.  {\color{red} I have issues with the next sentence that need to be discussed}.  We should note that using the usual GLS estimator in this context is not possible as $\bm{V}$ is not identifiable as shown by \cite{WicEtAl2019} who provide the alternative solutions that follow.

    \item Set ${\bm{W}}_{h}=k_{h}\text{diag}(\hat{\bm{W}}_{1})$ for all $h$, where $k_{h} > 0$ and
        $$
        \hat{\bm{W}}_{1} = \frac{1}{T}\sum_{T=1}^{T} \hat{\bm{e}}_{t}\hat{\bm{e}}_{t}'
        $$
        is the unbiased sample estimator of the in-sample one-step-ahead base forecast errors as defined in~\eqref{eq:base errors}. Hence this estimator scales the base forecasts using the variance of the in-sample residuals and is therefore described and referred to as a weighted least squares (WLS) estimator applying variance scaling. A similar estimator was proposed by \cite{Hyndman2016}.
        
        An alternative WLS estimator is proposed by \citet{AthEtAl2017} in the context of temporal hierarchies. Here $\bm{W}_{h}$ is proportional to $\text{diag}(\bm{S}\bm{1})$ where $\bm{1}$ being a unit column vector of dimension $n$.  Here weights are proportional to the number of bottom level variables required to form an aggregate. For example in the hierarchy in Figure~\ref{fig:simple tree} the weights corresponding to the Total, series A and series B are proportional to 5, 2 and 3 respectively.
        %An alternative diagonal scaling estimator is to set $\bm{W}_{h}=k_{h}\bm{\Lambda}$, for all $h$, where $k_{h} > 0$ and $\bm{\Lambda}=\text{diag}(\bm{S}\bm{1})$ with $\bm{1}$ being a unit column vector of dimension $n$. This was proposed by \citet{AthEtAl2017} for temporal hierarchies and assumes that each of the bottom-level base forecast errors has a variance $k_{h}$ and are uncorrelated between nodes. Each element of the diagonal $\bm{\Lambda}$ matrix contains the number of forecast error variances contributing to that aggregation level. 
        This weighting scheme depends only on the aggregation structure and is referred to as structural scaling. Its advantage over OLS is that it assumes equivariant forecast errors only at the bottom-level of the structure and not across all levels. It is particularly useful in cases where forecast errors are not available; for example, in cases where the base forecasts are generated by judgemental forecasting.

    \item  Set $\bm{W}_{h}=k_{h}\hat{\bm{W}}_{1}$, for all $h$, where $k_{h} > 0$, the unrestricted sample covariance estimator for $h=1$. Although this is relatively simple to obtain and provides a good solution for small hierarchies, it does not provide reliable results as $m$ grows compared to $T$. This is referred to this as the MinT(Sample) estimator.
    \item Set $\bm{W}_{h}=k_{h}\hat{\bm{W}}_{1}^D$, for all $h$, where $k_{h} > 0$, $\hat{\bm{W}}^{D}_{1} = \lambda_{D} \text{diag}(\hat{\bm{W}}_{1}) + (1 - \lambda_{D})\hat{\bm{W}}_{1}$ is a shrinkage estimator with diagonal target, and shrinkage intensity parameter

        $$\hat{\lambda}_{D} = \frac{\sum_{i \ne j}\hat{Var}(\hat{r}_{ij})}{\sum_{i \ne j}\hat{r}_{ij}^2},$$

        %$$\hat{\lambda}_{D} = \frac{\sum_{i \neq j} \widehat{\var(\hat{r}_{ij})}} {\sum_{i \neq j} \hat{r}_{ij}^{2}},$$

        where $\hat{r}_{ij}$ is the $ij$th element of $\hat{\bm{R}}_{1}$, the $1$-step-ahead sample correlation matrix as proposed by \citet{Schafer2005}. Hence, off-diagonal elements of $\hat{\bm{W}}_1$ are shrunk towards zero while diagonal elements (variances) remain unchanged. This is referred to as the MinT(Shrink) estimator.
\end{itemize}

\section{Hierarchical probabilistic forecasting}

A limitation of point forecasts is that they provide no indication of uncertainty around the forecast. A richer description of forecast uncertainty can be obtained by providing a ``probabilistic forecasts'', that is a full density for the target of interest. For a review of probabilistic forecasts, and methods for evaluating such forecasts known as {\em scoring rules} see  \citep{Gneiting2014}. In recent years, the use of probabilistic forecasts and their evaluation via scoring rules has become pervasive in macroeconomic forecasting, for example {\color{red} need to find some references that use scoring rules for macro forecasting.  Check Bayesian macro guys like Koop Korobilis, Josh Chan also Mike Smith's work with Shaun Vahey}.

%For example, \citet{McSharry2005} produced probabilistic forecasts for electricity demand, \citet{BenTaieb2017} for smart meter data, \citet{Pinson2009} for wind power generation, and \citet{Gel2004}, \citet{Gneiting2005a} and \citet{Gneiting2005} for various weather variables.

The literature on hierarchical probabilistic forecasting is still an emerging area of interest. %There's is only a few studies in published literature. The uncertainly around $\bm{y}_{T+h|T}$ is referred to as the probabilistic forecasts in hierarchical time series. Due to the aggregation nature of the data, these should also lie in the coherent subspace $\mathfrak{s}$. If so, we call them as coherent probabilistic forecasts.
To the best of our knowledge the first attempt to even define coherence in the setting of probabilistic forecasting is provided by \cite{Taieb2017} who define a coherent forecast in terms of a convolution.  An equivalent definition, provided by \cite{Gamakumara2018} defines a  coherent probabilistic forecast as a probability measure on the coherent subspace $\mathfrak{s}$.  \cite{Gamakumara2018} also generalise the concept of forecast reconciliation to the probabilistic setting.

\begin{definition} Let $\mathcal{A}$ be a subset\footnote{Strictly speaking $\mathcal{A}$ is a Borel set} of $\mathfrak{s}$ and let $\mathcal{B}$ be all points in $\mathbb{R}^n$ that are mapped onto  $\mathcal{A}$ after premultiplication by $\bm{S}\bm{G}$. Letting $\hat{\nu}$ be a `base' probabilistic forecast for the full hierarchy, the coherent measure $\tilde{\nu}$ reconciles $\hat{\nu}$ if $\tilde{\nu}(\mathcal{A})=\hat{\nu}(\mathcal{B})$ for all $\mathcal{A}$.
\end{definition}

In practice this definition leads to two approaches.  For some parametric distributions, for instance the multivariate normal,  a reconciled probabilistic forecast can be derived analytically.  However, in macroeconomic forecasting, non-standard distributions such as bimodal distributions are often required to take different policy regimes into account.  In such cases a non-parametric approach based on bootstrapping in-sample errors proposed \cite{Gamakumara2018} can be used.  These scenarios are now covered in detail.

\subsection{Probabilistic forecast reconciliation in the Gaussian framework}\label{sec:probrecgauss}

%Suppose we have a set of hierarchical time series where each realisation follows a multivariate Guassian distribution. i.e., $\bm{y}_T \sim \mathscr{N}(\bm{\mu}_T, \Sigma_T)$ where both $\bm{\mu}_T$ and $\Sigma_T$ lives in the coherent subspace $\mathfrak{s}$ due to the aggregation structure of the hierarchy. We are interested in estimating the predictive Gaussian distribution of $\bm{Y}_{T+h}| \bm{\mathscr{I}}_T$ where $\bm{\mathscr{I}}_T= \{\bm{y}_1,\bm{y}_2,\dots.,\bm{y}_T\}$, which should also lives in $\mathfrak{s}$. Since Gaussian distributions are uniquely characterised by the first two moments, it is sufficient to have the mean and variance forecasts to get the Gaussian predictive distributions of the hierarchy.

%Assume we have fit the time series models for each series of the hierarchy by considering all the available information. Using these fitted models we can estimate the means and variance forecasts of the hierarchy which are denoted by $\bm{\hat{\mu}}_{T+h}$ and $\hat{\Sigma}_{T+h}$ respectively. Each element in $\bm{\hat{\mu}}_{T+h}$ corresponds to the mean forecast of each series in the hierarchy and these elements are stacked in the same order as $\bm{y}_t$. Similarly $\hat{\Sigma}_{T+h}$ contains variances and covariances of all series in the hierarchy. One can either fit univariate models for each individual series of the hierarchy or fit multivariate models by considering the correlation structure of the series for getting these forecasts. However as long as the aggregation structure is not imposed, it is very unlikely that these forecasts will be coherent. Thus it comes to the point of reconciliation.

In the case where the base forecasts are probabilistic forecasts characterised by elliptical distributions \cite{Gamakumara2018} show that reconciled probabilistic forecasts will also be elliptical.  This is particularly straightforward for the Gaussian distribution which is completely characterised by two moments.  Letting the base probabilistic forecast be $\mathscr{N}(\bm{\hat{y}}_{T+h|T}, \hat{\bm{\Sigma}}_{T+h|T})$, then the reconciled probabilistic forecast will be $\mathscr{N}(\bm{\tilde{y}}_{T+h|T}, \tilde{\bm{\Sigma}}_{T+h|T})$, where,

\begin{equation}\label{eq:rec mean}
\bm{\tilde{y}}_{T+h|T} = \bm{SG}\bm{\hat{y}}_{T+h|T},
\end{equation}
and
\begin{equation}\label{eq:rec var}
\tilde{\bm{\Sigma}}_{T+h|T} = \bm{SG}\hat{\bm{\Sigma}}_{T+h|T}\bm{G'S'}.
\end{equation}

There are several options for obtaining the base probabilistic forecast and in particular the variance covariance matrix $\hat{\bm{\Sigma}}$.  One option is to fit multivariate models either level by level or for the hierarchy as a whole leading respectively to a $\hat{\bm \Sigma}$ that is block diagonal or dense.  Another alternative is to fit univariate models for each individual series in which case $\hat{\bm{\Sigma}}$ is a diagonal matrix. A third alternative, that we employ here, is to obtain $\hat{\bm{\Sigma}}$ using in-sample forecast errors, in a similar vein to how $\hat{\bm{W}}_{1}$ is estimated in the MinT method.  Here the same shrinkage estimator described in Section~\ref{sec:reconciliation approaches} is used.  The reconciled probabilistic forecast will ultimately depend on the choice of $\bm{G}$; the same choices of $\bm{G}$ matrices used in section~\ref{sec:point forecasting} can be used.

\subsection{Probabilistic forecast reconciliation in the non-parametric framework}\label{sec:probrecnp}

In many applications, including macroeconomic forecasting, it may not reasonable to assume Gaussian predictive distributions. Therefore, non-parametric approaches has been widely used for probabilistic forecasts in different disciplines. For example, ensemble forecasting in weather applications (\cite{Gneiting2005}, \cite{Gneiting2014}, \cite{Gneiting2008}), bootstrap based approaches (\cite{Manzan2008}, \cite{Vilar2013}). {\color{red} Check/replace these references with references that show heavy tails/skewness in macro applications.}

%Non-parametric approaches are also important in hierarchical forecasting as in most applications they have millions of time series which are often difficult to assume parametric distributions. Further, for data with heavy tails, it is often misleading to assume Gaussianity. The algorithm introduced by \cite{Taieb2017} is also a non-parametric approach as it does not make any distributional assumptions. They first generate, a sample from the bottom level predictive distribution, and then aggregate to obtain coherent probabilistic forecasts of the upper levels of the hierarchy. Initially they use MinT algorithm to reconcile the means of the bottom level forecast distributions, and then a copula-based approach is employed to model the dependency structure of the hierarchy. Resulting multi-dimensional distribution is used to generate the empirical forecast distributions for all bottom-level series which are then aggregated to obtain the empirical forecast distribution for the entire hierarchy. Although the means of forecast distributions are reconciled, the predictive distributions were obtained through a bottom-up based approach. Therefore this approach is not a reconciliation method as it does not use all the information from the hierarchy when producing coherent probabilistic forecasts.

%\cite{Jeon2018} is the only existing study that does reconciliation in probabilistic hierarchical forecasts. This method is based on cross-validation and it also does not assume any parametric distributions for predictive densities. However they applied this method particularly for temporal hierarchies.

Due to these concerns, we employ a reconciliation method proposed by \cite{Gamakumara2018} that does not make parametric assumptions about the predictive distribution.  An important result that this method exploits is that applying methods for point forecast reconciliation to the draws from incoherent base predictive distribution results in a sample from the reconciled predictive distribution. This process, is summarised

%In the first step we obtain possible sample paths from the incoherent forecast distributions. This follows by the reconciliation step which projects each sample path to the coherent subspace. These steps will be discussed in detail below.

\begin{enumerate}
	\item Fit univariate models to each series in the hierarchy over a training set from $t=1$ to $t=T$
	\item For each series compute $h$-step ahead point forecasts, for all $h$ up to $H$. Collect these into a $n\times H$ matrix $\hat{\bm Y}:=(\hat{\bm{y}}_{T+1|T},\ldots,\hat{\bm{y}}_{T+H|T})$, where $\hat{\bm{y}}_{T+h|T}$ is a $n\times 1$ vector of $h$-step point forecasts for all series in the hierarchy.
	\item Compute one-step ahead in-sample forecasting errors. Collect these into an $n \times T$ matrix ${\hat{\bm E}}=(\hat{\bm{e}}_1,\hat{\bm{e}}_2,.....,\hat{\bm{e}}_T)$, where the $n\times 1$ vector $\hat{\bm{e}}_t={\bm y}_t-\hat{\bm {y}}_{t|t-1}$.  Here, $\hat{\bm {y}}_{t|t-1}$ is a vector of forecasts made for time $t$ using information up to and including $t-1$. These are called in-sample forecasts since they depend on past values but information from the entire training sample is used to estimate parameters that forecasts are based on.
	\item Block bootstrap from $\hat{\bm{E}}$, that is choose $H$ consecutive columns of $\hat{{\bm E}}$ at random, repeating this process $B$ times.  Denote the $n\times H$ matrix obtained at iteration $b$ as $\hat{{\bm E}}^b$ for b=1,\ldots,B.
	\item For all $b$, compute $\hat{\bm \Upsilon}^b:=\hat{\bm Y}+{\bm \hat{\bm{E}}}^b$. Each row of $\hat{\bm \Upsilon}^b$ is a sample path of $h$ forecasts for a single series.  Each column of $\hat{\bm \Upsilon}^b$ is a realisation from the joint predictive distribution at a particular horizon.
	\item For each $b=1,\ldots,B$ select the $h^{th}$ column of $\hat{\bm \Upsilon}^b$ and stack these to form a $n\times B$ matrix $\hat{\bm{\Upsilon}}_{T+h|T}$
	\item For a given ${\bm G}$ matrix and for each $h=1,\ldots,H$ compute $\tilde{\bm{\Upsilon}}_{T+h|T}={\bm S}{\bm G}\hat{\bm{\Upsilon}}_{T+h|T}$.   Each column of $\tilde{\bm \Upsilon}_{T+h|T}$ is a realisation from the joint $h$-step ahead reconciled predictive distribution.
\end{enumerate}


%For a given forecast horizon and add these to the point forecasts $\hat{\bm{y}}_{T+1},\ldots,\hat{\bm{y}}_{T+h}$.  Repeating this process for $N$ bootstrap samples and stacking in an $(N \times n)$ gives,

%\begin{equation} \label{eq:19}
%\hat{\bm{Y}}^b_{T+h|T}=\begin{pmatrix}
%\hat{\bm{y}}_{1,T+h|T}^b\\
%\hat{\bm{y}}_{2,T+h|T}^b\\
%\vdots\\
%\hat{\bm{y}}_{N,T+h|T}^b
%\end{pmatrix}.
%\end{equation}



%bootstrapped errors will be incorporated as the error series for simulating future paths. Taking block bootstrapped in-sample errors in generating future paths will implicitly model the dependency structure of the hierarchy. These simulated future sample paths will be then formed in a vector $\hat{\bm{y}}_{T+h|T}^b$ by stacking the sample paths of each node in the same order as $\bm{y}_t$. As such we generate a sample of $N$ future paths for the hierarchy. We can denote this sample by a $(N \times n)$ matrix $\hat{\bm{Y}}^b_{T+h|T}$ where,

%\begin{equation} \label{eq:19}
%\hat{\bm{Y}}^b_{T+h|T}=\begin{pmatrix}
%\hat{\bm{y}}_{1,T+h|T}^b\\
%\hat{\bm{y}}_{2,T+h|T}^b\\
%\vdots\\
%\hat{\bm{y}}_{N,T+h|T}^b
%\end{pmatrix}.
%\end{equation}

%An advantage to this approach is that the dependence structure of the hierarchy as well as serial correlation, are captured through the bootstrapping.  However future paths constructed in this way will not be coherent and thus require reconciliation.

%\subsection*{Step 2: Reconciling future sample paths}

%The second step is to project these sample paths to the coherent subspace. Then we get a set of reconciled future paths which form a possible sample from the reconciled probabilistic forecast distribution. Similar to the projection used in the point forecast reconciliation, we project each sample path through the projection $\bm{SG}$. Then we get,

%\begin{equation} \label{eq:20}
%\tilde{\bm{y}}_{i,T+h}^b = \bm{SG}\hat{\bm{y}}_{i,T+h}^b, \quad i = 1, ..., N
%\end{equation}
%and let
%\begin{equation}\label{eq:21}
%\tilde{\bm{Y}}^b_{T+h|T}=\begin{pmatrix}
%\tilde{\bm{y}}_{1,T+h|T}^b\\
%\tilde{\bm{y}}_{2,T+h|T}^b\\
%\vdots\\
%\tilde{\bm{y}}_{N,T+h|T}^b
%\end{pmatrix}
%\end{equation}
%where, $\tilde{\bm{y}}_{i,T+h}^b \in \mathfrak{s}$ denote a $h$-step-ahead reconciled future paths for $i=1,...,N$. $\tilde{\bm{Y}}^b_{T+h|T}$ form an empirical coherent forecast distribution of the hierarchy that lies in the coherent subspace $\mathfrak{s}$. As we discussed in point forecast reconciliation methods, different estimates of $\bm{G}$ provide alternative estimates of reconciled future paths. We can also obtain bottom-up based future paths by simply aggregating the future paths of bottom level series for their respective upper levels. This is referred to as bottom-up future paths. Even though the bottom-up approach generates coherent probabilistic forecasts, it cannot be considered as a reconciliation method since it use only half of the information.

\section{Empirical Study: Australian GDP}\label{sec:data}

In our empirical data we consider Gross Domestic Product (GDP) of Australia with quarterly data spanning the period 1984:Q4-2018:Q3. The Australian Bureau of Statistics (ABS) measures GDP using three main approaches namely, Production, Income and Expenditure. The final GDP figure is obtained as an average of these three figures. Each of these measures are aggregates of economic variables which are also targets of interests for the macroeconomic forecaster. This suggests a hierarchical approach to forecasting could be used to improve forecasts of all series in the hierarchy including headline GDP.

We concentrate on the Income and Expenditure approaches as nominal data are available only for these two. We focus only on  nominal data due to the fact that real data are constructed via a chain price index approach with different price deflators used for each series. As a result, real GDP data are not coherent - the aggregate series is not a linear combination of the disaggregate series.  For similar reasons we do not use seasonally adjusted data; the process of seasonal adjustment results in data that are not coherent.  Finally, although there is a small statistical discrepancy between each series and the headline GDP figure, we simply treat this statistical discrepancy, which is also published by the ABS, as a time series in its own right. For further details on the data we refer the reader to \cite{ABS2018}.

\subsubsection*{Income approach}

Using the income approach, GDP is calculated by aggregating all income flows. In particular, GDP at purchaser's price is the sum of all factor incomes and taxes, minus subsidies on production and imports \citep{ABS2015}:
	\begin{align*}
	\textit{GDP} &= \textit{Compensation of employees} + \textit{Gross operating surplus}\\ &+ \textit{Gross mixed income}+ \textit{Taxes on production and imports}\\ &- \textit{Subsidies on production and imports} + \textit{Statistical discrepancy (I)}\\
	\end{align*}
Figure \ref{fig:GDP_I} shows the full hierarchical structure capturing all components aggregated to form GDP using the income approach. The hierarchy has two levels of aggregation below the top-level, with a total of $n=16$ series across the whole structure and $m=10$ series at the bottom-level.

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{Figs/IncomeApproach.png}
	\caption{Hierarchical structure of the income approach for GDP. The pink cell contains GDP the most aggregate series. The blue cells contain intermediate-level series and the yellow cells to the most disaggregate bottom-level series.}\label{fig:GDP_I}
\end{figure}

\subsubsection*{Expenditure approach}

In the expenditure approach, GDP is calculated as the aggregation of final consumption expenditure, gross fixed capital formation (GFCF), changes in inventories of finished goods, work-in-progress and raw materials and the value of exports less imports of the goods and services \citep{ABS2015}. Underline equation is,

\begin{align*}
	\textit{GDP} &= \textit{Final consumption expenditure} + \textit{Gross fixed capital formation} \\ & + \textit{Changes in inventories} + \textit{Trade balance}+\textit{Statistical discrepancy (E)}\\
	\end{align*}
Figures \ref{fig:GDP_E}, \ref{fig:GFCF} and \ref{fig:HFCE} show the full hierarchical structure capturing all components aggregated to form GDP using the expenditure approach. The hierarchy has three levels of aggregation below the top-level, with a total of $n=80$ series across the whole structure and $m=53$ series at the bottom-level.

\textcolor{orange}{A description of each series in these hierarchies along with the series ID assigned by the ABS is given in the tables \ref{Tab: Income-hierarchy}, \ref{Tab:Expenditure-hierarchy-1}, \ref{Tab:Expenditure-hierarchy-2} and \ref{Tab:Expenditure-hierarchy-3} in the supplementary materials.}


Figure \ref{fig:TSplots-INC-EXP} displays time series from the income and expenditure approaches. Top panel shows the most aggregate GDP series. The bottom panel shows series from below levels for income hierarchy in left panel and expenditure hierarchy in right panel. The plots show the diverse features of the time series with some displaying positive and others negative trending behaviour, some showing no trends but possibly a cycle, and some having a strong seasonal component. These highlight the need to account and model all information and diverse signals from each series in the hierarchy which can only be achieved through a forecast reconciliation approach.

\begin{figure}[t]
	\centering	\includegraphics[width=\textwidth]{Figs/ExpenditureApproach.png}
	\caption{Hierarchical structure of the expenditure approach for GDP. The pink cell contains GDP, the most aggregate series. The blue and purple cells contain intermediate-level series with the series in the purple cells further disaggregated in Figures \ref{fig:GFCF} and \ref{fig:HFCE}. The yellow cells contain the most disaggregate bottom-level series.}\label{fig:GDP_E}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{Figs/GFCF.png}
	\caption{Hierarchical structure for Gross Fixed Capital Formations under the expenditure approach for GDP, continued from Figure \ref{fig:GDP_E}. Blue cells contain intermediate-level series and the yellow cells to the most disaggregate bottom-level series.}\label{fig:GFCF}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{Figs/HFCE.png}
	\caption{Hierarchical structure for Household Final Consumption Expenditure under expenditure approach for GDP, continued from Figure \ref{fig:GDP_E}. Blue cells contain intermediate-level series and the yellow cells to the most disaggregate bottom-level series..}\label{fig:HFCE}
\end{figure}



\begin{figure}[t]
	\centering
	\small
	\includegraphics[width = \textwidth]{Figs/TS-plots/TSplots-INC-EXP.png}
	\caption{Time plots for series from different levels of income and expenditure hierarchies.}\label{fig:TSplots-INC-EXP}
\end{figure}


\section{Methodology}\label{sec:meth}

We now demonstrate the potential for reconciliation methods to improve forecast accuracy for Australian GDP data.  We consider forecasts from $h=1$ quarter ahead forecasts up to $h=4$ quarter ahead using an {\em expanding} window.  First, the training sample is set to Q4 of 1984 to Q3 of 1994 and forecasts are produced for Q4 of 1994 to Q3 of 1995. Then the training window is expanded one period ahead, i.e. from Q4 of 1984 to Q4 of 2017 with forecasts produces for the last observation at Q1 of 2018. All up this leads to 94 1-step-ahead, 93 2-step-ahead, 92 3-step-ahead and 91 4-step-ahead forecasts.


\subsection{Models}


The first task in forecasting reconciliation is to obtain base forecasts for all series in the hierarchy.  In the case of the income approach this necessitates forecasting $n=16$ separate time series while in the case of the expenditure approach forecasts for $n=80$ separate time series must be obtained.  Given the diversity in these time series discussed in Section~\ref{sec:data}, our focus was on a methodology that was fast but flexible.  We consider simple univariate ARIMA models, where model order is selected via a combination of unit root testing and AIC using an algorithm developed by XXX and implement in the auto.arima function in XXX. {\color{red} Cite this to Rob's satisfaction}.  A similar approach was also undertaken using the ETS framework to produce base forecasts.  This had minimal impact on our conclusions with respect to forecast reconciliation methods, and in most cases ARIMA forecats outperformed ETS forecasts.  {\color{orange} Consequently, results for ETS models are excluded but are available from the authors upon request}.  We note that a number of more complicated approaches could have been used to obtain base forecasts including multivariate models such as vector autoregression and models and methods that handle a large number of predictors such as factor models or least angle regresssion.  However, \cite{PanEtAl2019} show that univariate ARIMA models are highly competitive for forecasting Australian GDP even compared to these methods, and in any case our primary motivation is to demonstrate the potential of forecast reconciliation.

The forecast reconciliation approaches that we consider are bottom up, OLS, WLS with variance scaling and the MinT (shrink) approach.  The MinT (sample) approach was also used but due to the size of the hierarchy forecasts reconciled via this approach were less stable.  Finally, all forecasts both base and reconciled are compared to a \naive benchmark.  Since the data are not deseasonalised, the \naive benchmark is a seasonal random walk, i.e. the forecast for GDP (or one of its components) is the realised GDP in the same quarter of the previous year.  The \naive forecast is by construction coherent and therefore does not need to be reconciled.

\subsection{Evaluation}

For evaluating point forecasts we consider two metrics, the Mean Squared Error (MSE) and the Mean Absolute Scaled Error (MASE).  The absolute scaled error is defined as
\begin{equation*}
q_{T+h} = \sum \frac{|\breve{e}_{T+h|T}|}{(T-4)^{-1}\sum_{t=5}^{T}|y_t - y_{t-4}|}\,,
\end{equation*}
where $\breve{e}_{t+h}$ is the difference between any forecast and the realisation\footnote{Breve is used instead of a hat or tilde to denote that this can be the error either a base or reconciled forecast} and $4$ is used due to the quarterly nature of the data we consider.  An advantage of using MASE is that it is a scale independent measure. This is particularly relevant for hierarchical time series, since aggregate series by their very nature are on a larger scale than disaggregate series.  As such scale dependent metrics may unfairly favour methods that perform well for the aggregate series but poorly for disaggregate series.  For more details on different point forecast accuracy measures refer to \cite[Chapter 3 of][]{HynAth2018}.

Forecast accuracy of probabilistic forecasts can be evaluated using scoring rules \citep{Gneiting2014}.  Let $\breve{F}$ be a probabilistic forecast and let $\breve{\bm{y}}\sim \breve{F}$  where breve is used to denote that either base forecast or reconciled forecast can be evaluated.  The accuracy of multivariate probabilistic forecasts will be measured by the energy score given by
\begin{equation*}
eS(\breve{F}_{T+h|T},\bm{y}_{T+h}) =
E_{\breve{F}}\|\breve{\bm{y}}_{T+h}-\bm{y}_{T+h}\|^\alpha
-\frac{1}{2}E_{\breve{F}}\|\breve{\bm{y}}_{T+h}-\breve{\bm{y}}^*_{T+h}\|^\alpha\,,
\end{equation*} where $\bm{y}_{T+h}$ is the realisation at time $T+h$, $\alpha\in (0,2]$. We set $\alpha=1$, noting that other values of $\alpha$ give similar results.  The expectations can be evaluated numerically as long as a sample from $\breve{F}$ is available which is the case for all methods we employ.  An advantage of using energy score is that in the univariate case it simplifies to the commonly used cumulative rank probability score (CRPS) given by

\begin{equation*} 
\text{CRPS}(\breve{F}_i,y_{i,T+h}) = E_{\breve{F}_i}|\breve{y}_{i,T+h}-y_{i,T+h}| - \frac{1}{2}E_{\breve{F}_i}|\breve{y}_{i,T+h}-\breve{y}^*_{i,T+h}|,
\end{equation*}

where the subscript $i$ is used to denote that CRPS measures forecast accuracy for a single variable in the hierarchy.

As an alternative to the energy score, log score and variogram scores were also considered.  The log score was disregarded since \cite{Gamakumara2018} prove that the log score is improper with respect to the class of incoherent probabilistic forecasts when the true DGP is coherent.  {\color{orange}The variogram score gave similar results to the energy score; variogram score results are omitted for brevity but are available from the authors upon request.}


%\subsubsection{Comparison between different forecasting methods}
%
%{\color{red}To be included in results section - and discussed much more briefly.}
%
%We are mainly interested to evaluate the hierarchical forecasts in two aspects. One is to examine whether the having coherent forecasts is improving the forecast accuracy. This can be evaluated by comparing the incoherent forecasts with any coherent forecasts in both point as well as probabilistic framework. Secondly we are interested in finding the best reconciliation method by comparing reconciled forecast from different reconciliation methods.
%
%For any comparison we use Skill score as defined in \citep{Gneiting2007}. For a given forecasting method, evaluated by a particular scoring rule $S(\cdot)$ , the skill score can be calculated as follows,
%\begin{equation} \label{eq:25}
%Ss[S_B(\cdot)] = \frac{S_B(\bm{Y},\bm{y})^{\text{ref}} - S_B(\breve{\bm{Y}},\bm{y})}{S_B(\bm{Y},\bm{y})^{\text{ref}}}\times 100\%,
%\end{equation}
%where $S_B(\cdot)$ is average score over $B$ replicates and $S_B(\bm{Y},\bm{y})^{\text{ref}}$ is the average score of the reference forecasting methods. Thus $Ss[S_B(\cdot)]$ gives the percentage improvement of the preferred forecasting method relative to the reference method. Any positive value indicates that method is superior to the reference method, whereas any negative value of $Ss[S_B(\cdot)]$ indicate that the method we compared is poor than the reference method.
%
%
%
%
%First step in the reconciliation process is to generate base forecasts in both point and probabilistic frameworks. Thus we fit ETS and ARIMA models for each individual series of the hierarchy by using default settings in forecast package in R software implemented by \citep{Hyndman2018}. We further use seasonal naive forecasts as the benchmark which are always coherent.


%\subsubsection*{Income approach}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/INC-PointF.png}
%	\caption{Summary of point forecasts in income approach}\label{Exp-PointF}
%\end{figure}
%
%
%\subsubsection*{Expenditure approach}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/EXP-PointF.png}
%	\caption{Summary of point forecasts in expenditure approach}\label{Inc-PointF}
%\end{figure}




\section{Results}\label{sec:results}

\subsection{Base forecasts}

Due to the different features in each time series a variety of ARIMA models were selected to be used as base models.  {\color{red} Some generalisations about models}.  Figure~\ref{fig: NaiveVsBase_MSE} gives some indication of the performance of these base forecasting models relative to the \naive forecast over different horizons.  Panels on the left refer to results for the Income hierarchy with panels on the right referring to the expenditure hierarchy.  The top panels summarise results over all series in the hierarchy.  Here we have computed MSE for each series, the results in Figure~\ref{fig: NaiveVsBase_MSE} are averages over all series. The clear result is that base forecasts are more accurate than \naive forecasts, however as the forecasting horizon increases, the difference becomes smaller.  This is to be expected since the \naive model here is a seasonal random walk, for horizons $h<4$ the forecast from an ARIMA model is based on more recent information.

Similar results are obtained when MASE is used as the metric for evaluating forecast quality.  However one disadvantage of the base forecasts relative to the \naive forecast is that base forecasts are no longer coherent.  As such we now turn out attention to investigating whether reconciliation methods can lead to further improvements in forecast accuracy relative to base forecasts.

%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[width=\textwidth]{Figs/Results/NaiveVsBase_SS_MSE.png}
%	\caption{Skill scores for naive forecasts (relative to base forecasts) using mean square error as a forecast metric.  Top panels refer to results summarised over all series, bottom panels only refer to the top-level GDP series.  Left panels refer to the income hierarchy, right panels to the expenditure hierarchy}\label{fig: NaiveVsBase_SS_MSE}
%\end{figure}

\begin{figure}[t]
	\centering
	\small
	\includegraphics[width=\textwidth]{Figs/Results/NaiveVsBase_MSE.png}
	\caption{Mean squared errors for naive and base forecasts. Top panels refer to results summarised over all series and bottom panels refer to the top-level GDP series.  Left panels refer to the income hierarchy and right panels to the expenditure hierarchy}\label{fig: NaiveVsBase_MSE}
\end{figure}


%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/NaiveVsBase_MASE.png}
%	\caption{Skill scores for naive forecasts based on MASE}\label{NaiveVsBase_MASE}
%\end{figure}

\subsection{Point Forecast Reconciliation}


We now turn our attention to an evaluation of point forecasts obtained using different reconciliation methods.  All results in subsequent figures are presented as the percentage changes in a forecasting metric relative to the base forecast, a measure known in the forecasting literature as {\em skill scores}.  Skill scores are computed such that positive values represent an improvement in forecasting performance over the base forecast and negative values represent a deterioration in forecast quality.  The top row of Figure \ref{fig: PointF} shows skill scores based on the MSE and MASE averaged over all series.  We conclude that reconciliation methods generally improve forecast accuracy relative to base forecasts regardless of the hierarchy used, the forecasting horizon, the forecasting metric used to evaluate forecasts or the specific reconciliation method employed.  We do however note that while all reconciliation methods improve forecast performance, MinT (shrink) is the best forecasting method in most cases.

To further investigate the differences between reconciliation methods we break down these results by different levels of each hierarchy.  The second row of Figure \ref{fig: PointF} shows the forecasting performance a single series, namely GDP which represents the top level of both hierarchies.  The third row shows results for all series excluding those on the bottom level, while the final row shows results for the bottom level series only.  Here, we see two general features, the first is that OLS reconciliation performs poorly on the bottom level series, the second is that bottom up performs relatively poorly on aggregate series.  These two features are particularly exacerbated for the larger expenditure hierarchy.  These results are consistent with other findings in the forecast reconciliation literature see for instance XXX. {\color{red} Some papers from forecast reconciliation literature to be cited here}


\begin{figure}[t]
	\centering
	\small
	\includegraphics[width=\textwidth]{Figs/Results/PointF.png}
	\caption{Skill score point forecasts from different reconciliation methods (with reference to base forecasts). Left two panels refer to skill score using MSE for income and expenditure hierarchies. Similarly right two panels refer to skill score using MASE. First row refers to results summarised over all series, second row to top-level GDP series, third row to aggregate levels and last row to the bottom level.}
	\label{fig: PointF}
\end{figure}




\subsection{Probabilistic Forecast Reconciliation}

We now turn our attention towards results for probabilistic forecasts.  Figure \ref{fig: Prob-forecasts-SS_ES} reports the energy score which as a multivariate score summarises forecast accuracy over the entire hierarchy.  Once again all results are presented as skill scores relative to base forecasts.  The top panels refer to assuming Gaussian probabilistic forecasts as described in Section~\ref{sec:probrecgauss} while the bottom panels refer to the method described in Section~\ref{sec:probrecnp}.  The left panels correspond to the income hierarchy, the right panels to the expenditure hierarchy.  For the income hierarchy, all methods improve upon base forecasts at all horizons.  In nearly all cases the best performing reconciliation method is MinT (shrink), a notable result since the optimal properties for MinT have thus far only been established theoretically in the point forecasting case.  For the larger expenditure hierarchy results are little more mixed.  While bottom up tends to perform poorly, all other reconciliation methods improve upon base forecasts (with the single exception of MinT (shrink) in the Gaussian framework four quarters ahead).  Interestingly, OLS performs best under the assumption of Gaussianity - this may indicate that OLS is a more robust method under model misspecification but further investigation is required.

Finally, Figure \ref{fig: Prob-forecasts-SS_CRPS} displays the skill scores based on the cumulative ranked probability score for a single series, namely top-level GDP.  The cause of the poor performance of bottom up reconciliation as a failure to accurately forecast aggregate series is apparent here.

\begin{figure}[t]
	\centering
	\small
	\includegraphics[width=\textwidth]{Figs/Results/ProbF_MultivS.png}
	\caption{Skill score for multivariate probabilistic forecasts from different reconciliation methods (with reference to base forecasts) using energy score. Top panels refer to the results for Gaussian approach and bottom panels to the non-parametric bootstrap approach. Left panels refer to the income hierarchy and right panels to the expenditure hierarchy.}
	\label{fig: Prob-forecasts-SS_ES}
\end{figure}

\begin{figure}[t]
	\centering
	\small
	\includegraphics[width=\textwidth]{Figs/Results/ProbF_UnivS.png}
	\caption{Skill score for probabilistic forecasts of top-level GDP from different reconciliation methods (with reference to base forecasts) using CRPS. Top panels refer to the results for Gaussian approach and bottom panels refer to the non-parametric bootstrap approach. Left panel refers to the income hierarchy and right panel to the expenditure hierarchy.}
	\label{fig: Prob-forecasts-SS_CRPS}
\end{figure}

%Results are presented in tables \ref{Tab: Inc_ProbGaus_ES_VS}, \ref{Tab: Inc_ProbGaus_LS}, \ref{Tab: Inc_ProbGaus_UnivS} for income approach and \ref{Tab: Exp_ProbGaus_ES_VS}, \ref{Tab: Exp_ProbGaus_LS}, \ref{Tab: Exp_ProbGaus_UnivS} for expenditure approach. \\





%\subsection*{Income approach}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/INC-ProbGaussF-MultivS_ES_VS.png}
%	\caption{Skill scores with respect to energy score and variogram score for multivariate Gaussian forecast distribution of income hierarchy}\label{Inc_ProbGaus_ES_VS}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/INC-ProbGaussF-UnivS_CRPS.png}
%	\caption{Skill scores for univariate Gaussian forecast distributions of individual series of income hierarchy}\label{Inc_ProbGaus_UnivS}
%\end{figure}
%
%
%\subsection*{Expenditure approach}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/EXP-ProbGaussF-MultivS_ES_VS.png}
%	\caption{Skill scores with respect to energy score and variogram score for multivariate Gaussian forecast distribution of expenditure hierarchy}\label{Exp_ProbGaus_ES_VS}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/EXP-ProbGaussF-UnivS_CRPS.png}
%	\caption{Skill scores for univariate Gaussian forecast distributions of individual series of expenditure hierarchy}\label{Exp_ProbGaus_UnivS}
%\end{figure}
%
%
%
%
%
%
%
%
%
%
%\subsubsection{Non-parametric probabilistic forecasts for Australian GDP}
%
%We also estimate the coherent probabilistic forecasts for GDP and its disaggregate components by using the non-parametric bootstrap approach explained in the section (?).
%
%\subsection*{Income approach}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/INC-ProbNonParaF-MultivS_ES_VS.png}
%	\caption{Skill scores with respect to energy score and variogram score for multivariate Gaussian forecast distribution of income hierarchy}\label{Inc_ProbNonParF_ES_VS}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/INC-ProbNonParaF-UnivS_CRPS.png}
%	\caption{Skill scores for univariate Gaussian forecast distributions of individual series of income hierarchy}\label{Inc_ProbNonParF_UnivS}
%\end{figure}
%
%
%
%
%
%
%\subsection*{Expenditure approach}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/EXP-ProbNonParaF-MultivS_ES_VS.png}
%	\caption{Skill scores with respect to energy score and variogram score for multivariate Gaussian forecast distribution of expenditure hierarchy}\label{Exp_ProbNonParF_ES_VS}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.50]{Figs/Results/EXP-ProbNonParaF-UnivS_CRPS.png}
%	\caption{Skill scores for univariate Gaussian forecast distributions of individual series of expenditure hierarchy}\label{Exp_ProbNonParF_UnivS}
%\end{figure}




\pagebreak

\section{Conclusions}

In the macroeconomic setting, we have demonstrated the potential for forecast reconciliation methods to not only provide coherent forecasts, but to also improve overall forecast accuracy.  This result holds for both point forecasts and probabilistic forecasts, for two different hierarchies and over different forecasting horizons.  Even where the objective is to only forecast a single series, for instance top-level GDP, the application of forecast reconciliation methods improves forecast accuracy.

By comparing results from different forecast reconciliation techniques we make a number of conclusions.  Despite its simplicity, bottom up can perform poorly at more aggregated levels of the hierarchy, a result found elsewhere in the literature. Meanwhile, when forecast accuracy at the bottom level is evaluated, OLS tends to break down in some instances.  Overall, the WLS and MinT (shrink) methods, and particularly the latter tend to yield the highest improvements in forecast accuracy.  

There are a number of open avenues for research in the literature on forecast reconciliation, some of which are particularly relevant to macroeconomic applications.  First there is scope to consider more complex aggregation structures, for instance in addition to the hierarchies we have already considered, data on GDP and GDP components disaggregated along geographical lines are also available.  This leads to a grouped hierarchy.  Also, given the substantial literature on the optimal frequency at which to analyse macroeconomic data, a study on forecasting GDP or other variables as a temporal hierarchy may be of interest.  In this chapter we have only shown that reconciliation methods can be used to improve forecast accuracy when univariate ARIMA models are used to produce base forecasts.  It will be interesting to evaluate whether such results hold when multivariate approaches, e.g. a Bayesian VAR or dynamic factor model, is used as a base forecasting model, or whether the gains from forecast reconciliation would be more modest.  Finally, a current limitation of the forecast reconciliation literature is that it only applies to collections of time series that adhere to linear constraints.  In macroeconomics there are many examples of data that adhere to non-linear constraints, for instance real GDP is a complicated but deterministic function of GDP components and price deflators.  The extension of forecast reconciliation methods to non-linear constraints potentially holds great promise for continued improvement in macroeconomic forecasting.
%\section*{Appendix}
%\addcontentsline{toc}{section}{Appendix 1}




%
%\begin{table}[t]
%	\caption{Variables, Series IDs and their descriptions for Income Approach}
%	\centering
%	\resizebox{\linewidth}{!}{
%		\begin{tabular}[t]{lll}
%			\toprule
%			\textbf{Variable} & \textbf{Series ID} & \textbf{Description}\\
%			\midrule
%			Gdpi & A2302467A & GDP(I)\\
%			Sdi & A2302413V & Statistical discrepancy (I)\\
%			Tsi & A2302412T & Taxes less subsidies (I)\\
%			TfiCoeWns & A2302399K & Compensation of employees; Wages and salaries\\
%			TfiCoeEsc & A2302400J & Compensation of employees; Employers' social contributions\\
%			\addlinespace
%			TfiCoe & A2302401K & Compensation of employees\\
%			TfiGosCopNfnPvt & A2323369L & Private non-financial corporations; Gross operating surplus\\
%			TfiGosCopNfnPub & A2302403R & Public non-financial corporations; Gross operating surplus\\
%			TfiGosCopNfn & A2302404T & Non-financial corporations; Gross operating surplus\\
%			TfiGosCopFin & A2302405V & Financial corporations;  Gross operating surplus\\
%			\addlinespace
%			TfiGosCop & A2302406W & Total corporations; Gross operating surplus\\
%			TfiGosGvt & A2298711F & General government; Gross operating surplus\\
%			TfiGosDwl & A2302408A & Dwellings owned by persons; Gross operating surplus\\
%			TfiGos & A2302409C & All sectors; Gross operating surplus\\
%			TfiGmi & A2302410L & Gross mixed income\\
%			Tfi & A2302411R & Total factor income\\
%			\bottomrule
%		\end{tabular}
%		\label{Tab: Income-hierarchy}
%	}
%\end{table}

%\begin{table}[t]
%	\caption{Variables, Series IDs and their descriptions for Expenditure Approach}
%	\centering
%	\resizebox{\linewidth}{!}{
%		\begin{tabular}[t]{lll}
%			\toprule
%			\textbf{Variable} & \textbf{Series ID} & \textbf{Description}\\
%			\midrule
%			
%			Gdpe & A2302467A & GDP(E)\\
%			Sde & A2302566J & Statistical Discrepancy(E)\\
%			Exp & A2302564C & Exports of goods and services\\
%			Imp & A2302565F & Imports of goods and services\\
%			Gne & A2302563A & Gross national exp.\\
%			\addlinespace
%			GneDfdFceGvtNatDef & A2302523J & Gen. gov. - National; Final consumption exp. - Defence\\
%			GneDfdFceGvtNatNdf & A2302524K & Gen. gov. - National; Final consumption exp. - Non-defence\\
%			GneDfdFceGvtNat & A2302525L & Gen. gov. - National; Final consumption exp.\\
%			GneDfdFceGvtSnl & A2302526R & Gen. gov. - State and local; Final consumption exp,\\
%			GneDfdFceGvt & A2302527T & Gen. gov.; Final consumption exp.\\
%			\addlinespace
%			GneDfdFce & A2302529W & All sectors; Final consumption exp.\\
%			GneDfdGfcPvtTdwNnu & A2302543T & Pvt.; Gross fixed capital formation (GFCF)\\
%			GneDfdGfcPvtTdwAna & A2302544V & Pvt.; GFCF - Dwellings - Alterations and additions\\
%			GneDfdGfcPvtTdw & A2302545W & Pvt.; GFCF - Dwellings - Total\\
%			GneDfdGfcPvtOtc & A2302546X & Pvt.; GFCF - Ownership transfer costs\\
%			\addlinespace
%			GneDfdGfcPvtPbiNdcNbd & A2302533L & Pvt. GFCF - Non-dwelling construction - New building\\
%			GneDfdGfcPvtPbiNdcNec & A2302534R & Pvt.; GFCF - Non-dwelling construction -\\
%			&  & New engineering construction\\
%			GneDfdGfcPvtPbiNdcSha & A2302535T & Pvt.; GFCF - Non-dwelling construction -\\
%			&  & Net purchase of second hand \vphantom{1} assets\\
%			\addlinespace
%			GneDfdGfcPvtPbiNdc & A2302536V & Pvt.; GFCF - Non-dwelling construction - Total\\
%			GneDfdGfcPvtPbiNdmNew & A2302530F & Pvt.; GFCF - Machinery and equipment - New\\
%			GneDfdGfcPvtPbiNdmSha & A2302531J & Pvt.; GFCF - Machinery and equipment -\\
%			&  & Net purchase of second hand assets\\
%			GneDfdGfcPvtPbiNdm & A2302532K & Pvt.; GFCF - Machinery and equipment - Total\\
%			\addlinespace
%			GneDfdGfcPvtPbiCbr & A2716219R & Pvt.; GFCF - Cultivated biological resources\\
%			GneDfdGfcPvtPbiIprRnd & A2716221A & Pvt.; GFCF - Intellectual property products -\\
%			&  & Research and development\\
%			GneDfdGfcPvtPbiIprMnp & A2302539A & Pvt.; GFCF - Intellectual property products -\\
%			&  & Mineral and petroleum exploration\\
%			\addlinespace
%			GneDfdGfcPvtPbiIprCom & A2302538X & Pvt.; GFCF - Intellectual property products - Computer software\\
%			GneDfdGfcPvtPbiIprArt & A2302540K & Pvt.; GFCF - Intellectual property products - Artistic originals\\
%			GneDfdGfcPvtPbiIpr & A2716220X & Pvt.; GFCF - Intellectual property products Total\\
%			GneDfdGfcPvtPbi & A2302542R & Pvt.;  GFCF - Total private business investment\\
%			GneDfdGfcPvt & A2302547A & Pvt.; GFCF\\
%			\addlinespace
%			GneDfdGfcPubPcpCmw & A2302548C & Plc. corporations - Commonwealth; GFCF\\
%			GneDfdGfcPubPcpSnl & A2302549F & Plc. corporations - State and local; GFCF\\
%			GneDfdGfcPubPcp & A2302550R & Plc. corporations; GFCF Total\\
%			GneDfdGfcPubGvtNatDef & A2302551T & Gen. gov. - National; GFCF - Defence\\
%			GneDfdGfcPubGvtNatNdf & A2302552V & Gen. gov. - National ; GFCF - Non-defence\\
%			\addlinespace
%			GneDfdGfcPubGvtNat & A2302553W & Gen. gov. - National ; GFCF Total\\
%			GneDfdGfcPubGvtSnl & A2302554X & Gen. gov. - State and local; GFCF\\
%			GneDfdGfcPubGvt & A2302555A & Gen. gov.; GFCF\\
%			GneDfdGfcPub & A2302556C & Plc.; GFCF\\
%			GneDfdGfc & A2302557F & All sectors; GFCF\\
%			\bottomrule
%		\end{tabular}
%		
%		\label{Tab:Expenditure-hierarchy-1}
%	}
%\end{table}
%
%\begin{table}[t]
%	\caption{Variables, Series IDs and their descriptions for Changes in Inventories - Expenditure Approach}
%	\centering
%	\resizebox{\linewidth}{!}{
%		\begin{tabular}[t]{lll}
%			
%			\toprule
%			\textbf{Variable} & \textbf{Series ID} & \textbf{Description}\\
%			\midrule
%			
%			GneCii & A2302562X & Changes in Inventories\\
%			GneCiiPfm & A2302560V & Farm\\
%			GneCiiPba & A2302561W & Public authorities\\
%			GneCiiPnf & A2302559K & Private; Non-farm Total\\
%			GneCiiPnfMin & A83722619L & Private; Mining (B)\\
%			\addlinespace
%			GneCiiPnfMan & A3348511X & Private; Manufacturing (C)\\
%			GneCiiPnfWht & A3348512A & Private; Wholesale trade (F)\\
%			GneCiiPnfRet & A3348513C & Private; Retail trade (G)\\
%			GneCiiPnfOnf & A2302273C & Private; Non-farm; Other non-farm industries\\
%			\bottomrule
%		\end{tabular}
%		\label{Tab:Expenditure-hierarchy-2}
%	}
%\end{table}
%
%\begin{table}[t]
%	\caption{Variables, Series IDs and their descriptions for Household Final Consumption - Expenditure Approach}
%	\centering
%	\resizebox{\linewidth}{!}{
%		\begin{tabular}[t]{lll}
%			
%			\toprule
%			\textbf{Variable} & \textbf{Series ID} & \textbf{Description}\\
%			\midrule
%			
%			GneDfdHfc & A2302254W & Household Final Consumption Expenditure\\
%			GneDfdFceHfcFud & A2302237V & Food\\
%			GneDfdFceHfcAbt & A3605816F & Alcoholic beverages and tobacco\\
%			GneDfdFceHfcAbtCig & A2302238W & Cigarettes and tobacco\\
%			GneDfdFceHfcAbtAlc & A2302239X & Alcoholic beverages\\
%			\addlinespace
%			GneDfdFceHfcCnf & A2302240J & Clothing and footwear\\
%			GneDfdFceHfcHwe & A3605680F & Housing, water, electricity, gas and other fuels\\
%			GneDfdFceHfcHweRnt & A3605681J & Actual and imputed rent for housing\\
%			GneDfdFceHfcHweWsc & A3605682K & Water and sewerage charges\\
%			GneDfdFceHfcHweEgf & A2302242L & Electricity, gas and other fuel\\
%			\addlinespace
%			GneDfdFceHfcFhe & A2302243R & Furnishings and household equipment\\
%			GneDfdFceHfcFheFnt & A3605683L & Furniture, floor coverings and household goods\\
%			GneDfdFceHfcFheApp & A3605684R & Household appliances\\
%			GneDfdFceHfcFheTls & A3605685T & Household tools\\
%			GneDfdFceHfcHlt & A2302244T & Health\\
%			\addlinespace
%			GneDfdFceHfcHltMed & A3605686V & Medicines, medical aids and therapeutic appliances\\
%			GneDfdFceHfcHltHsv & A3605687W & Total health services\\
%			GneDfdFceHfcTpt & A3605688X & Transport\\
%			GneDfdFceHfcTptPvh & A2302245V & Purchase of vehicles\\
%			GneDfdFceHfcTptOvh & A2302246W & Operation of vehicles\\
%			\addlinespace
%			GneDfdFceHfcTptTsv & A2302247X & Transport services\\
%			GneDfdFceHfcCom & A2302248A & Communications\\
%			GneDfdFceHfcRnc & A2302249C & Recreation and culture\\
%			GneDfdFceHfcEdc & A2302250L & Education services\\
%			GneDfdFceHfcHcr & A2302251R & Hotels, cafes and restaurants\\
%			\addlinespace
%			GneDfdFceHfcHcrCsv & A3605694V & Catering services\\
%			GneDfdFceHfcHcrAsv & A3605695W & Accommodation services\\
%			GneDfdFceHfcMis & A3605696X & Miscellaneous goods and services\\
%			GneDfdFceHfcMisOgd & A3605697A & Other goods\\
%			GneDfdFceHfcMisIfs & A2302252T & Insurance and other financial services\\
%			GneDfdFceHfcMisOsv & A3606485T & Other services\\
%			\bottomrule
%		\end{tabular}
%		\label{Tab:Expenditure-hierarchy-3}
%	}
%\end{table}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/INC-hierarchy/INC-aggregates_TSplots.png}
%	\caption{All aggregate level series of income hierarchy.}\label{INC-aggregates_TSplot}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/INC-hierarchy/INC-bottomlevel-TSplots.png}
%	\caption{All bottom level series of income hierarchy.}\label{INC-bottomlevel_TSplot}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-1.png}
%	\caption{GDP(E), GNE, Experts less Imports and Statistical discrepancy in expenditure hierarchy.}\label{EXP-set-1}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-2.png}
%	\caption{Domestic Final Demand, Final Consumption Expenditure and Gross Fixed Capital Formations in expenditure hierarchy.}\label{EXP-set-2}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-3.png}
%	\caption{Total changes in inventory, Private non-farm, Private farm and Public authorities in expenditure hierarchy.}\label{EXP-set-3}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-4.png}
%	\caption{Disaggregation of private non-farm in expenditure hierarchy.}\label{EXP-set-4}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-5.png}
%	\caption{Disaggregation of government final consumption expenditure.}\label{EXP-set-5}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.65]{Figs/TS-plots/EXP-hierarchy/set-6.png}
%	\caption{Public, private and total fixed capital formations.}\label{EXP-set-6}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-7.png}
%	\caption{Disaggregation of general government of Gross fixed capital formations.}\label{EXP-set-7}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.65]{Figs/TS-plots/EXP-hierarchy/set-8.png}
%	\caption{Disaggregation of public corporations of Gross fixed capital formations.}\label{EXP-set-8}
%\end{figure}
%
%\begin{figure}[t]
% 	\centering
% 	\small
% 	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-9.png}
% 	\caption{Disaggregation of total dwelling and ownership transfer.}\label{EXP-set-9}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-10.png}
%	\caption{Main disaggregation of total private business investments.}\label{EXP-set-10}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-11.png}
%	\caption{Remaining disaggregation of total private business investments.}\label{EXP-set-11}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-12.png}
%	\caption{Main disaggregation of household final consumption expenditure.}\label{EXP-set-12}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-13.png}
%	\caption{Disaggregation of household final consumption expenditure.}\label{EXP-set-13}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-14.png}
%	\caption{Disaggregation of household final consumption expenditure.}\label{EXP-set-14}
%\end{figure}
%
%\begin{figure}[t]
%	\centering
%	\small
%	\includegraphics[scale=0.5]{Figs/TS-plots/EXP-hierarchy/set-15.png}
%	\caption{Disaggregation of household final consumption expenditure.}\label{EXP-set-15}
%\end{figure}

\pagebreak
\bibliographystyle{agsm}

\bibliography{References_BookChapter_HTS,library}


\end{document}
